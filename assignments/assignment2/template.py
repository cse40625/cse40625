import numpy as np
import pandas as pd


def accuracy_score(y_true, y_pred):
    """Accuracy classification score.

    Parameters
    ----------
    y_true : array, shape = [n_instances]
        Actual target values.
    y_pred : array, shape = [n_instances]
        Predicted target values.

    Returns
    -------
    score : float
        Returns the fraction of correctly classified instances.
    """
    score = y_true == y_pred
    return np.average(score)


def sigmoid(z):
    """Computes the sigmoid function."""
    return 1. / (1. + np.exp(-1. * z))


class LogisticRegression(object):
    """Logistic regression (also known as logit or MaxEnt) classifier.

    Logistic regression is a supervised linear classifier that uses the sigmoid
    function to "squash" a linear input, generated by combining the feature
    vector with a set of weights, to produce a bounded output that can be
    interpreted as an estimate of the genuine probability of a binary outcome.

    This implementation optimizes the model using gradient descent. The
    gradient used to update the weight vector is computed based on [1].

    Parameters
    ----------
    max_iter : int, optional (default=500)
        Maximum number of iterations taken for the solver to converge.
    learning_rate : float, optional (default=0.01)
        The learning rate for weight updates.
    random_state : int or None, optional (default=None)
        If int, random_state is the seed used by the random number generator.
        If None, the random number generator is the RandomState instance used
        by np.random.

    Attributes
    ----------
    weight_ : array
        Weight vector.

    References
    ----------
    .. [1] Y. S. Abu-Mostafa, M. Magdon-Ismail, and H-T Lin. "Learning from
           Data." AMLBook, 2012.
    """

    def __init__(self, max_iter=500, learning_rate=0.01, random_state=None):
        self.max_iter = max_iter
        self.learning_rate = learning_rate
        self.random_state = random_state

        self.weight_ = None

    def _compute_gradient(self, X, y):
        """Compute the gradient for the logistic regression error.

        Logistic regression minimizes binomial cross-entropy error:
            E(w) = 1/N \sum_N ln(1 + exp(y_n * w.T \dot x_n)),
        which can be minimized by computing the gradient:
            grad = -1/N \sum_N ((y_n * x_n) / (1 + exp(y_n * w.T \dot x_n))),
        where N is the number of instances, w are the weights, and the dot
        product is denoted by \dot.

        Parameters
        ----------
        X : array, shape = [n_instances, n_features + 1]
            Training data.
        y : array, shape = [n_instances, n_classes]
            Target values (assumed to be {-1, 1}).

        Returns
        -------
        grad : array, shape = [n_weights,]
            Gradient of the error.
        """
        # ================ YOUR CODE HERE ================
        # Instructions: Return the gradient of the error.
        # ================================================

    def _update_weight(self, grad):
        """Updates the weight vector.

        Given the learning rate and gradient of the error, the updated weight
        vector w can be computed as:
            w = learning_rate * -grad
        which adjusts the weight vector in the direction of negative error.

        Parameters
        ----------
        grad : array, shape = [n_weights,]
            Gradient of the error.
        """
        # ================ YOUR CODE HERE ================
        # Instructions: Update the weight vector.
        # ================================================

    def _decision_function(self, X):
        """Decides target value for instance(s).

        The predicted target value for each instance x of X is computed as:
            theta(w.T \dot x)
        where theta is the logistic function, w is the weight vector, x is the
        feature vector or matrix, and the dot product is denoted by \dot.

        Parameters
        ----------
        X : array, shape = [n_instances, n_features + 1]
            Feature matrix or vector for instance(s) to update.

        Returns
        -------
        P : float
            Predicted target value for instance(s).
        """
        # ================ YOUR CODE HERE ================
        # Instructions: Return the predicted target value.
        # ================================================

    def fit(self, X, y):
        """Fit logistic regression.

        Parameters
        ----------
        X : array, shape = [n_instances, n_features]
            Training data.
        y : array, shape = [n_instances, n_classes]
            Target values (assumed to be {-1, 1}).

        Returns
        -------
        self : Returns an instance of self.
        """
        np.random.seed(seed=self.random_state)

        # ================ YOUR CODE HERE ================
        # Instructions: Insert a negative bias term and initialize the weights
        # with random samples from a Normal distribution with zero mean and
        # unit variance. Iterate up to max_iter times. Each iteration, compute
        # the gradient (_compute_gradient) and use it to update the weight
        # vector (_update_weight) in the direction of the greatest decrease in
        # error. Using the updated weight vector, apply the decision function
        # to generate the predicted target class probabilities, using a
        # threshold on these probabilities to predict the target class values
        # (predict). Each iteration, print the current model accuracy. Halt if
        # no misclassified instances remain.
        # ================================================

    def predict(self, X):
        """Predict target values for instances in X.

        Parameters
        ----------
        X : array, shape = [n_instances, n_features]
            Instances.

        Returns
        -------
        y_pred : array, shape = [n_instances,]
            Predicted target value for instances.
        """
        # ================ YOUR CODE HERE ================
        # Instructions: Insert a negative bias term. Use the probabilities
        # output by the decision function to decide the predicted (binary)
        # target value. Use a probability of 0.5 as the threshold, with
        # positive (+1) class predictions assigned to predicted probabilites
        # greater than 0.5 and negative (-1) class predictions assigned to
        # predicted probabilities less than 0.5.
        # ================================================


if __name__ == "__main__":
    with open('output.txt', 'w') as f_out:
        df = pd.read_csv('digits_binary.csv')
        X = df.iloc[:, :-1]
        y = df.iloc[:, -1]

        clf = LogisticRegression(random_state=0)
        clf.fit(X, y)
